# Enhancing Stress Detection on Social Media Through Multi-Modal Fusion of Text and Synthesized Visuals

[![Paper](https://img.shields.io/badge/Paper-BioNLP%202025-blue)](link-to-paper-if-available)  
[![Conference](https://img.shields.io/badge/Venue-BioNLP%202025-9cf)](https://aclanthology.org/events/biolp-2025/)  

This repository contains the code and experimental setup for the paper:  

> **Efstathia Soufleri and Sophia Ananiadou.**  
> *Enhancing Stress Detection on Social Media Through Multi-Modal Fusion of Text and Synthesized Visuals.*  
> Proceedings of the 24th Workshop on Biomedical Language Processing (BioNLP 2025), pp. 34‚Äì43.  

---

## üìñ Overview
Stress detection on social media is a critical task for understanding population-level mental health signals. While most approaches rely solely on textual data, our work explores **multi-modal fusion** of:
- **Textual representations** from pre-trained language models, and  
- **Synthesized visuals** generated from social media posts.  

We show that integrating visual signals‚Äîeven when synthesized rather than user-provided‚Äîimproves performance on stress detection benchmarks, highlighting the potential of multi-modal methods in low-resource mental health applications.

---

## üõ†Ô∏è Features
- End-to-end training pipeline for **multi-modal stress detection**  
- Support for **fusion architectures** (early, late, hybrid)  
- Scripts for **visual synthesis** of text using image generation models  
- Evaluation on **social media stress datasets**  

---

